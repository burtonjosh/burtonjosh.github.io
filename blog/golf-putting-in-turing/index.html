<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.4 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/minimal-mistakes.css">
<link rel="stylesheet" href="/css/adjust.css">
<link rel="icon" href="/assets/temp_fav.png">
<!--[if IE ]>
<style>
  /* old IE unsupported flexbox fixes */
  .greedy-nav .site-title {
    padding-right: 3em;
  }
  .greedy-nav button {
    position: absolute;
    top: 0;
    right: 0;
    height: 100%;
  }
</style>
<![endif]-->

   <title>Golf putting</title>  
  <!-- end custom head snippets -->
  <meta name="google-site-verification" content="83wzCvn1Z2WBAj5oNN1RlvnWS5Xb5MmekUS2wgld9sg" />
</head>
<body class="layout--single">
  <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Explorations in inference</a>
        <ul class="visible-links">
          <li class="masthead__menu-item"><a href="/blog/" >Blog</a></li>
          <li class="masthead__menu-item"><a href="/work/" >Publications & Awards</a></li>
          <li class="masthead__menu-item"><a href="/joshua_burton_cv.pdf" >CV</a></li>
          <li class="masthead__menu-item"><a href="/hobbies/">Other Interests</a></li>
        </ul>
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

  <div class="initial-content">
    <div id="main" role="main">
      <div class="sidebar sticky">
        <div itemscope itemtype="https://schema.org/Person">
          <div class="author__avatar">
            <img src="/assets/portrait.jpg" alt="Me" itemprop="image">
          </div>
          <div class="author__content">
            <h3 class="author__name" itemprop="name">Joshua Burton</h3>
            <p class="author__bio" itemprop="description">PhD in Mathematical Biology, team member at PlantingSpace, 
              software developer, general purpose AI systems and Bayesian inference researcher.</p>
          </div>
          <div class="author__urls-wrapper">
            <button class="btn btn--inverse">Follow</button>
            <ul class="author__urls social-icons">
              <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
                <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Manchester, United Kingdom</span></li>
              <li><a href="https://twitter.com/josh_burt0n" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
              <li><a href="https://github.com/burtonjosh" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
              <li><a href="https://scholar.google.co.uk/citations?user=y5cJwpEAAAAJ&hl=en" rel="nofollow noopener noreferrer"><i class="fa fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
              <li><a href="mailto:josh@burton-online.me.uk" rel="nofollow noopener noreferrer"><i class="fa fa-envelope" aria-hidden="true"></i> Email</a></li>
              <!-- <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li> -->
              <!-- <i class="fa-brands fa-orcid"></i> -->
            </ul>
          </div>
        </div>
      </div>


<!-- Content appended here -->
<div class="franklin-content"><h1 id="posterior_predictive_checking_with_turing_and_arviz_using_the_golf_putting_case_study"><a href="#posterior_predictive_checking_with_turing_and_arviz_using_the_golf_putting_case_study" class="header-anchor">Posterior predictive checking with Turing and ArviZ, using the golf putting case study</a></h1>
<p>The <a href="https://mc-stan.org/users/documentation/case-studies/golf.html">golf putting case study</a> in the Stan documentation is a really nice showcase of <em>iterative model building</em> in the context of MCMC. In this post I&#39;m going to demonstrate how the models in the Stan tutorial can be implemented with <a href="https://turing.ml/stable/">Turing</a>, a Julia library for general-purpose probabilistic programming. This has already been done by Joshua Duncan<sup id="fnref:jduncan"><a href="#fndef:jduncan" class="fnref">[1]</a></sup>, so additionally I&#39;ll show how to make use of the <a href="https://julia.arviz.org/stable/">ArviZ.jl</a> package in order to do some posterior predictive analysis, specifically PSIS-LOO cross validation and LOO-PIT predictive checking. These are really powerful tools which can be used to help us evaluate our model fit, but I&#39;ll focus only on the implementation with Turing and ArviZ, so if you want to understand these tools and how to interpret them you can read about PSIS-LOO <a href="https://arxiv.org/pdf/1507.04544.pdf">here</a>, and LOO-PIT in Gelman et al. BDA &#40;2014&#41;, Section 6.3.</p>
<p>For full details on each of the models, I suggest reading the case study in the Stan documentation, which is really nicely written and easy to follow.</p>
<div class="franklin-toc"><ol><li><a href="#data_visualisation">Data visualisation</a></li><li><a href="#models">Models</a><ol><li><a href="#logistic_regression">Logistic regression</a><ol><li><a href="#fitting_the_model">Fitting the model</a></li><li><a href="#visualising_the_fit">Visualising the fit</a></li><li><a href="#evaluating_the_fit_using_elpd">Evaluating the fit using ELPD</a></li></ol></li><li><a href="#modelling_from_first_principles">Modelling from first principles</a><ol><li><a href="#posterior_predictive_checks">Posterior predictive checks</a></li></ol></li><li><a href="#an_extended_model_that_accounts_for_how_hard_the_ball_is_hit">An extended model that accounts for how hard the ball is hit</a><ol><li><a href="#a_bad_fit">A bad fit</a></li><li><a href="#a_good_fit">A good fit</a></li></ol></li></ol></li><li><a href="#model_selection">Model selection</a></li><li><a href="#conclusion">Conclusion</a></li><li><a href="#references">References</a></li></ol></div>
<h2 id="data_visualisation"><a href="#data_visualisation" class="header-anchor">Data visualisation</a></h2>
<p>The first thing to do is have a look at the data we have and see if we get any inspiration for a model.</p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Turing, DelimitedFiles, StatsPlots, StatsFuns, ArviZ
gr(size=(<span class="hljs-number">580</span>,<span class="hljs-number">301</span>))

<span class="hljs-comment"># Turing can have quite verbose output, so I&#x27;ll</span>
<span class="hljs-comment"># suppress that for readability</span>
<span class="hljs-comment"># It&#x27;s usually a good idea to not do this,</span>
<span class="hljs-comment"># so you are warned of any divergences</span>
<span class="hljs-keyword">import</span> Logging
Logging.disable_logging(Logging.Warn); <span class="hljs-comment"># or e.g. Logging.Info</span></code></pre>
<pre><code class="julia hljs">data = readdlm(<span class="hljs-string">&quot;_assets/blog/golf-putting-in-turing/code/golf_data_old.txt&quot;</span>)
x, n, y = data[:,<span class="hljs-number">1</span>], data[:,<span class="hljs-number">2</span>], data[:,<span class="hljs-number">3</span>];
yerror = sqrt.(y./n .* (<span class="hljs-number">1</span> .-y./n)./n)

scatter(x,y ./ n, yerror = yerror,
        ylabel = <span class="hljs-string">&quot;Probability of success&quot;</span>,
        xlabel = <span class="hljs-string">&quot;Distance from hole (feet)&quot;</span>,
        title = <span class="hljs-string">&quot;Data on putts in pro golf&quot;</span>,
        legend=:<span class="hljs-literal">false</span>,
        color=:lightblue,
        )</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/output/initial_data.svg" alt="">
<p>The error bars associated with each point <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span> in the plot are given by <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mrow><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>j</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>j</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><msub><mi>n</mi><mi>j</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\hat{p}_j(1−\hat{p}_j)/n_j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.3231em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9169em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">p</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">p</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8769em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119
c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120
c340,-704.7,510.7,-1060.3,512,-1067
l0 -0
c4.7,-7.3,11,-11,19,-11
H40000v40H1012.3
s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232
c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1
s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26
c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60z
M1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3231em;"><span></span></span></span></span></span></span></span></span>, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>j</mi></msub><mo>=</mo><msub><mi>y</mi><mi>j</mi></msub><mi mathvariant="normal">/</mi><msub><mi>n</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\hat{p}_j=y_j/n_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">p</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> is the success rate for putts taken at distance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>.</p>
<p>We can see a pretty clear trend – the greater the distance from the hole, the lower the probability of the ball going in.</p>
<h2 id="models"><a href="#models" class="header-anchor">Models</a></h2>
<p>It&#39;s typically a good idea to start with a very simple model, and then slowly add in more features to improve the fit of our models. This is exactly what is done in the Stan documentation. One of the simplest models we can use in this instance is a logistic regression.</p>
<blockquote>
<p>📝 I&#39;ll go through all of the details for defining the model, fitting it, and doing posterior checks in this first example, but in the later ones I&#39;ll just show the code and end result.</p>
</blockquote>
<h3 id="logistic_regression"><a href="#logistic_regression" class="header-anchor">Logistic regression</a></h3>
<p>The first model considers the probability of success &#40;i.e. getting the ball in the hole&#41; as a function of distance from the hole, using a logistic regression:</p>
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>∼</mo><mtext>Binomial</mtext><mo stretchy="false">(</mo><msub><mi>n</mi><mi>j</mi></msub><mo separator="true">,</mo><msup><mtext>logit</mtext><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="1em"/><mrow><mtext>for j = 1,</mtext><mo>…</mo><mtext>, J.</mtext></mrow></mrow><annotation encoding="application/x-tex"> y_j \sim \text{Binomial}(n_j, \text{logit}^{-1}(a + bx_j)), \hspace{1em}\text{for j = 1,\dots, J.} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1846em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">Binomial</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord text"><span class="mord">logit</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8984em;"><span style="top:-3.1473em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal">b</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">))</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">for j = 1,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">, J.</span></span></span></span></span></span>
<p>In Turing, this looks like</p>
<pre><code class="julia hljs"><span class="hljs-meta">@model</span> <span class="hljs-keyword">function</span> golf_logistic(x, y, n)
    a ~ Normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)
    b ~ Normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)

    p = logistic.(a .+ b .* x)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:length(n)
        y[i] ~ Binomial(n[i],p[i])
    <span class="hljs-keyword">end</span>
<span class="hljs-keyword">end</span></code></pre>
<pre><code class="plaintext hljs">golf_logistic (generic function with 2 methods)</code></pre>
<h4 id="fitting_the_model"><a href="#fitting_the_model" class="header-anchor">Fitting the model</a></h4>
<p>We can fit this model with the No-U-Turn sampler &#40;NUTS&#41; using Turing&#39;s <code>sample</code> function.</p>
<pre><code class="julia hljs">logistic_model = golf_logistic(x,y,n)
fit_logistic = sample(logistic_model,NUTS(),MCMCThreads(),<span class="hljs-number">2000</span>,<span class="hljs-number">4</span>)</code></pre>
<pre><code class="plaintext hljs">Chains MCMC chain (2000×14×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 0.82 seconds
Compute duration  = 0.75 seconds
parameters        = a, b
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ess_per_sec
      Symbol   Float64   Float64   Float64     Float64     Float64   Float64       Float64

           a    2.2245    0.0583    0.0013   1985.5510   2633.9384    1.0031     2640.3604
           b   -0.2551    0.0067    0.0001   2065.1743   2646.8013    1.0025     2746.2424

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol   Float64   Float64   Float64   Float64   Float64

           a    2.1098    2.1857    2.2236    2.2651    2.3372
           b   -0.2681   -0.2597   -0.2552   -0.2506   -0.2420
</code></pre>
<p>Here we&#39;ve computed 8000 posterior samples across 4 chains. The MCSE of the mean is 0 to two decimal places for both parameters, and both <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> values are close to 1, indicating convergence and good mixing between chains. So let&#39;s have a look at the fit.</p>
<h4 id="visualising_the_fit"><a href="#visualising_the_fit" class="header-anchor">Visualising the fit</a></h4>
<p>The following plot shows the model fit. The black line corresponds to the posterior median, and the grey lines show 500 draws from the posterior distribution.</p>
<pre><code class="julia hljs"><span class="hljs-comment"># get posterior median</span>
meda, medb = [quantile(<span class="hljs-built_in">Array</span>(fit_logistic)[:,i],<span class="hljs-number">0.5</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">2</span>]

<span class="hljs-comment"># select some posterior samples at random</span>
n_samples = <span class="hljs-number">500</span>
fit_samples = <span class="hljs-built_in">Array</span>(fit_logistic)
idxs = sample(axes(fit_samples,<span class="hljs-number">1</span>),n_samples)
posterior_draws = fit_samples[idxs,:]

posterior_data = [logistic.(posterior_draws[i,<span class="hljs-number">1</span>] .+ posterior_draws[i,<span class="hljs-number">2</span>].*x) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:n_samples]
plot(x,posterior_data,color=:grey,alpha=<span class="hljs-number">0.08</span>,legend=:<span class="hljs-literal">false</span>)

plot!(x,logistic.(meda .+ medb.*x),color=:black,linewidth=<span class="hljs-number">2</span>)
scatter!(x,y ./ n, yerror = yerror,
        ylabel = <span class="hljs-string">&quot;Probability of success&quot;</span>,
        xlabel = <span class="hljs-string">&quot;Distance from hole (feet)&quot;</span>,
        title = <span class="hljs-string">&quot;Fitted logistic regression&quot;</span>,
        legend=:<span class="hljs-literal">false</span>,
        color=:lightblue)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/output/logistic_plot.svg" alt="">
<p>A <em>by-eye</em> evaluation leads us to conclude this isn&#39;t a very good fit. Can we formalise this conclusion with some more rigorous tools?</p>
<h4 id="evaluating_the_fit_using_elpd"><a href="#evaluating_the_fit_using_elpd" class="header-anchor">Evaluating the fit using ELPD</a></h4>
<p>The expected log pointwise predictive density &#40;ELPD&#41; is a measure of predictive accuracy for each of the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi></mrow><annotation encoding="application/x-tex">J</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span> data points taken one at a time<sup id="fnref:elpd"><a href="#fndef:elpd" class="fnref">[2]</a></sup>. It can&#39;t be computed directly, but we can estimate it with leave-one-out &#40;LOO&#41; cross-validation. The LOO estimate of the ELPD can be noisy under certain conditions, but can be improved with Pareto smoothed importance sampling. This is the estimate implemented in ArviZ. In order to use the tools in ArviZ we first have to compute the posterior predictive distribution, save the pointwise log likelihoods from our model fit, and convert our Turing chains into an ArviZ <code>InferenceData</code> object.</p>
<blockquote>
<p>📝 For our predictive posterior we replace the data <code>y</code> with a vector of <code>missing</code> values, <code>similar&#40;y, Missing&#41;</code>.</p>
</blockquote>
<pre><code class="julia hljs"><span class="hljs-comment"># Instantiate the predictive model</span>
logistic_predict = golf_logistic(x,similar(y, <span class="hljs-built_in">Missing</span>),n)
posterior_predictive = predict(logistic_predict, fit_logistic)


<span class="hljs-comment"># Ensure the ordering of the loglikelihoods matches the ordering of `posterior_predictive`</span>
loglikelihoods = pointwise_loglikelihoods(logistic_model, fit_logistic)
names = string.(keys(posterior_predictive))
loglikelihoods_vals = getindex.(<span class="hljs-built_in">Ref</span>(loglikelihoods), names)

<span class="hljs-comment"># Reshape into `(nchains, nsamples, size(y)...)`</span>
loglikelihoods_arr = permutedims(cat(loglikelihoods_vals...; dims=<span class="hljs-number">3</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>));

<span class="hljs-comment"># construct idata object</span>
idata_logistic = from_mcmcchains(
    fit_logistic;
    posterior_predictive=posterior_predictive,
    log_likelihood=<span class="hljs-built_in">Dict</span>(<span class="hljs-string">&quot;ll&quot;</span> =&gt; loglikelihoods_arr),
    library=<span class="hljs-string">&quot;Turing&quot;</span>,
    observed_data=(; x, n, y)
);</code></pre>
<p>With this <code>InferenceData</code> object we can utilise some powerful analysis and visualisation tools in ArviZ.</p>
<p>For example, we can now easily estimate the ELPD with PSIS-LOO.</p>
<pre><code class="julia hljs">logistic_loo = loo(idata_logistic,pointwise=<span class="hljs-literal">true</span>);
println(<span class="hljs-string">&quot;LOO estimate is &quot;</span>,round(logistic_loo.elpd_loo[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>), <span class="hljs-string">&quot; with an SE of &quot;</span>,round(logistic_loo.se[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>),
        <span class="hljs-string">&quot; and an estimated number of parameters of &quot;</span>,round(logistic_loo.p_loo[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>))</code></pre>
<pre><code class="plaintext hljs">LOO estimate is -208.66 with an SE of 66.35 and an estimated number of parameters of 43.31
</code></pre>
<p>The estimate itself is relative, so it isn&#39;t very useful in isolation, but we can see that the standard error is high, and the estimated number of parameters in the model is ~40, much larger than the true value of 1, indicating severe model misspecification.</p>
<p>Another tool at our disposal is the PSIS-LOO probability integral transform. Oriol Abril has written a nice blog post on how to interpret the following plots<sup id="fnref:loopit"><a href="#fndef:loopit" class="fnref">[3]</a></sup>, and I&#39;ll quote directly from the post to give some intuition about this:</p>
<blockquote>
<p>Probability Integral Transform stands for the fact that given a random variable <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>, <strong>the random variable <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><msub><mi>F</mi><mi>X</mi></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo>≤</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y = F_X(X) = P(x\leq X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span> is a uniform random variable if the transformation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">F_X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the Cumulative Density Function</strong> &#40;CDF&#41; of the original random variable <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>. If instead of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">F_X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> we have <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> samples from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{x_1,\dots, x_n\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>, we can use them to estimate <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">\hat{F}_X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0968em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and apply it to future <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> samples <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">x^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span>. In this case, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mi>X</mi></msub><mo stretchy="false">(</mo><msup><mi>x</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{F}_X(x^∗)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1968em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> will be approximately a uniform random variable, converging to an exact uniform variable as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> tends to infinity.</p>
</blockquote>
<p>So we&#39;re looking for our computed <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">\hat{F}_X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0968em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> to be, more or less, a uniform random variable. Anything other than this suggests that the model is a poor fit.</p>
<blockquote>
<p>📝 This is not to say that if <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>F</mi><mo>^</mo></mover><mi>X</mi></msub></mrow><annotation encoding="application/x-tex">\hat{F}_X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0968em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is uniform then our model is good. Doing a large number of varied diagnostic checks, e.g <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>, ESS &#40;bulk and tail&#41;, MCSE, and posterior predictive checks like PSIS-LOO, simply gives us more opportunity to identify any issues with our model.</p>
</blockquote>
<p>The LOO-PIT for our simple logistic regression model is clearly not uniform which suggests to us, as expected, the model is a poor choice.</p>
<pre><code class="julia hljs">plot_loo_pit(idata_logistic; y=<span class="hljs-string">&quot;y&quot;</span>)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/../images/logistic_loo_pit.png" alt="">
<p>Sometimes it&#39;s not very clear if the estimate is non-uniform, so we can plot the difference between the LOO-PIT Empirical Cumulative Distribution Function &#40;ECDF&#41; and the uniform CDF instead of LOO-PIT kde for a more &#39;zoomed in&#39; look.</p>
<pre><code class="julia hljs">plot_loo_pit(idata_logistic; y=<span class="hljs-string">&quot;y&quot;</span>,ecdf=<span class="hljs-literal">true</span>)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/../images/logistic_loo_pit_ecdf.png" alt="">
<p>Lastly, it&#39;s a good idea to plot our Pareto <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> diagnostic, to assess the reliability of the above estimates<sup id="fnref:psisdiag"><a href="#fndef:psisdiag" class="fnref">[4]</a></sup>. In short, we&#39;re looking for values of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&lt;</mo><mn>0.7</mn></mrow><annotation encoding="application/x-tex">k &lt; 0.7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.7</span></span></span></span>. Any <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> values greater than 1 and we can&#39;t compute an estimate for the Monte Carlo standard error &#40;SE&#41; of the ELPD. In this case the ELPD estimate is not reliable. These high <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> values also explain the large estimate for the effective number of parameters.</p>
<pre><code class="julia hljs">plot_khat(logistic_loo)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/../images/logistic_khat.png" alt="">
<h3 id="modelling_from_first_principles"><a href="#modelling_from_first_principles" class="header-anchor">Modelling from first principles</a></h3>
<p>So hopefully we&#39;re pretty convinced by now that this isn&#39;t a good model. An alternative approach is to consider the angle of the shot, where 0 corresponds to the centre of the hole. We assume that the golfers attempt to hit the ball directly at the hole &#40;i.e. an angle of 0&#41;, but are not always perfect due to a number of &#40;known or unknown&#41; factors, so that the angle is Normally distributed about 0 with some standard deviation <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>. The model is derived in detail in the Stan documentation, so I will just show the Turing implementation.</p>
<pre><code class="julia hljs">Phi(x) = cdf(Normal(),x)

<span class="hljs-meta">@model</span> <span class="hljs-keyword">function</span> golf_angle(x,y,n,r,R)
    threshold_angle = asin.((R-r) ./ x)
    sigma ~ truncated(Normal(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>),<span class="hljs-number">0</span>,<span class="hljs-literal">Inf</span>)
    p = <span class="hljs-number">2</span> .* Phi.(threshold_angle ./ sigma) .- <span class="hljs-number">1</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:length(n)
        y[i] ~ Binomial(n[i],p[i])
    <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">return</span> sigma*<span class="hljs-number">180</span>/<span class="hljs-literal">π</span> <span class="hljs-comment"># generated quantities</span>
<span class="hljs-keyword">end</span>

r = (<span class="hljs-number">1.68</span> / <span class="hljs-number">2</span>) / <span class="hljs-number">12</span>;
R = (<span class="hljs-number">4.25</span> / <span class="hljs-number">2</span>) / <span class="hljs-number">12</span>;

angle_model = golf_angle(x,y,n,r,R)
fit_angle = sample(angle_model,NUTS(),MCMCThreads(),<span class="hljs-number">2000</span>,<span class="hljs-number">4</span>)</code></pre>
<pre><code class="plaintext hljs">Chains MCMC chain (2000×13×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 4.52 seconds
Compute duration  = 4.1 seconds
parameters        = sigma
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ess_per_sec
      Symbol   Float64   Float64   Float64     Float64     Float64   Float64       Float64

       sigma    0.0267    0.0004    0.0000   4009.7971   5280.0952    1.0005      977.5225

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol   Float64   Float64   Float64   Float64   Float64

       sigma    0.0259    0.0264    0.0266    0.0269    0.0274
</code></pre>
<p>The model is fit just as before, and if we visualise the fit against the posterior median of the previous model it seems to be a lot better.</p>
<pre><code class="julia hljs"><span class="hljs-comment"># get posterior median</span>
med_sigma = quantile(<span class="hljs-built_in">Array</span>(fit_angle),<span class="hljs-number">0.5</span>)

<span class="hljs-comment"># select posterior samples at random</span>
n_samples = <span class="hljs-number">500</span>
fit_samples = <span class="hljs-built_in">Array</span>(fit_angle)
idxs = sample(<span class="hljs-number">1</span>:length(fit_angle),n_samples)
posterior_draws = fit_samples[idxs]

threshold_angle = asin.((R-r) ./ x)

post_lines = [<span class="hljs-number">2</span> .* Phi.(threshold_angle ./ posterior_draws[i]) .- <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:n_samples]
plot(x,post_lines,color=:grey,alpha=<span class="hljs-number">0.08</span>,legend=:<span class="hljs-literal">false</span>)
plot!(x,<span class="hljs-number">2</span> .* cdf.(Normal(),threshold_angle ./ med_sigma) .- <span class="hljs-number">1</span>,color=:blue,linewidth=<span class="hljs-number">1.5</span>)

plot!(x,logistic.(meda .+ medb.*x),color=:black,linewidth=<span class="hljs-number">2</span>)
annotate!(<span class="hljs-number">12</span>,<span class="hljs-number">0.5</span>,<span class="hljs-string">&quot;Logistic regression model&quot;</span>,<span class="hljs-number">8</span>)
annotate!(<span class="hljs-number">5</span>,<span class="hljs-number">0.35</span>,text(<span class="hljs-string">&quot;Geometry based model&quot;</span>,color=:blue,<span class="hljs-number">8</span>))
scatter!(x,y ./ n, yerror = yerror,
        ylabel = <span class="hljs-string">&quot;Probability of success&quot;</span>,
        xlabel = <span class="hljs-string">&quot;Distance from hole (feet)&quot;</span>,
        title = <span class="hljs-string">&quot;Comparison of both models&quot;</span>,
        legend=:<span class="hljs-literal">false</span>,
        color=:lightblue)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/output/comparison_plot.svg" alt="">
<h4 id="posterior_predictive_checks"><a href="#posterior_predictive_checks" class="header-anchor">Posterior predictive checks</a></h4>
<p>Let&#39;s have a look at the same posterior predictive checks that we did for the previous model.</p>
<pre><code class="julia hljs"><span class="hljs-comment"># Instantiate the predictive model</span>
angle_predict = golf_angle(x,similar(y, <span class="hljs-built_in">Missing</span>),n,r,R)
posterior_predictive = predict(angle_predict, fit_angle)

<span class="hljs-comment"># Ensure the ordering of the loglikelihoods matches the ordering of `posterior_predictive`</span>
loglikelihoods = pointwise_loglikelihoods(angle_model, fit_angle)
names = string.(keys(posterior_predictive))
loglikelihoods_vals = getindex.(<span class="hljs-built_in">Ref</span>(loglikelihoods), names)
<span class="hljs-comment"># Reshape into `(nchains, nsamples, size(y)...)`</span>
loglikelihoods_arr = permutedims(cat(loglikelihoods_vals...; dims=<span class="hljs-number">3</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>));

idata_angle = from_mcmcchains(
    fit_angle;
    posterior_predictive=posterior_predictive,
    log_likelihood=<span class="hljs-built_in">Dict</span>(<span class="hljs-string">&quot;ll&quot;</span> =&gt; loglikelihoods_arr),
    library=<span class="hljs-string">&quot;Turing&quot;</span>,
    observed_data=(;x, n, y)
);

angle_loo = loo(idata_angle,pointwise=<span class="hljs-literal">true</span>);
println(<span class="hljs-string">&quot;LOO is &quot;</span>,round(angle_loo.elpd_loo[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>), <span class="hljs-string">&quot; with an SE of &quot;</span>,round(angle_loo.se[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>),
        <span class="hljs-string">&quot; and an estimated number of parameters of &quot;</span>,round(angle_loo.p_loo[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>))</code></pre>
<pre><code class="plaintext hljs">LOO is -89.31 with an SE of 13.04 and an estimated number of parameters of 8.4
</code></pre>
<pre><code class="julia hljs">plot_loo_pit(idata_angle; y=<span class="hljs-string">&quot;y&quot;</span>)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/../images/angle_loo_pit.png" alt="">
<p>This looks pretty promising. Our LOO-PIT seems to be uniform. However, when we look at the ECDF plot we can see there&#39;s some difference between our estimate and the uniform CDF.</p>
<pre><code class="julia hljs">plot_loo_pit(idata_angle; y=<span class="hljs-string">&quot;y&quot;</span>,ecdf=<span class="hljs-literal">true</span>)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/../images/angle_loo_pit_ecdf.png" alt="">
<p>We also have one problematic Pareto <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> value, rendering our estimate unreliable anyhow.</p>
<pre><code class="julia hljs">plot_khat(logistic_loo)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/../images/angle_khat.png" alt="">
<p>If this wasn&#39;t enough to convince you that the model isn&#39;t great, there is actually a lot more golf data available to us. Here I&#39;ll do something slightly different to the Stan documentation, and fit our model directly to this new data, to see how it performs.</p>
<pre><code class="julia hljs">data_new = readdlm(<span class="hljs-string">&quot;_assets/blog/golf-putting-in-turing/code/golf_data_new.txt&quot;</span>)
x_new, n_new, y_new = data_new[:,<span class="hljs-number">1</span>], data_new[:,<span class="hljs-number">2</span>], data_new[:,<span class="hljs-number">3</span>];
y = y_new./n_new
yerror_new = sqrt.(y .* (<span class="hljs-number">1</span> .-y)./n_new);

angle_model = golf_angle(x_new,y_new,n_new,r,R)
fit_angle = sample(angle_model,NUTS(),MCMCThreads(),<span class="hljs-number">2000</span>,<span class="hljs-number">4</span>)</code></pre>
<pre><code class="plaintext hljs">Chains MCMC chain (2000×13×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 0.83 seconds
Compute duration  = 0.8 seconds
parameters        = sigma
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std      mcse   ess_bulk    ess_tail      rhat   ess_per_sec
      Symbol   Float64   Float64   Float64    Float64     Float64   Float64       Float64

       sigma    0.0473    0.0000    0.0000   829.5659   1046.5808    1.0027     1034.3715

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol   Float64   Float64   Float64   Float64   Float64

       sigma    0.0473    0.0473    0.0473    0.0473    0.0473
</code></pre>
<pre><code class="julia hljs"><span class="hljs-comment"># get posterior median</span>
med_sigma = quantile(<span class="hljs-built_in">Array</span>(fit_angle),<span class="hljs-number">0.5</span>)

threshold_angle = asin.((R-r) ./ x_new)
plot(x_new,<span class="hljs-number">2</span> .* cdf.(Normal(),threshold_angle ./ med_sigma) .- <span class="hljs-number">1</span>,color=:blue,linewidth=<span class="hljs-number">1.5</span>)
scatter!(x_new,y, yerror = yerror_new,
        ylabel = <span class="hljs-string">&quot;Probability of success&quot;</span>,
        xlabel = <span class="hljs-string">&quot;Distance from hole (feet)&quot;</span>,
        title = <span class="hljs-string">&quot;New data, old model&quot;</span>,
        legend=:<span class="hljs-literal">false</span>,
        color=:lightblue)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/output/new_data.svg" alt="">
<p>And this really sends our posterior checks wild:</p>
<img src="/assets/blog/golf-putting-in-turing/code/../images/angle_loo_pit_new.png" alt="">
<img src="/assets/blog/golf-putting-in-turing/code/../images/angle_loo_pit_ecdf_new.png" alt="">
<img src="/assets/blog/golf-putting-in-turing/code/../images/angle_khat_new.png" alt="">
<p>Something is definitely not right here, and it&#39;s clear that this data can&#39;t be explained only by angular precision. Indeed, in golf you can&#39;t just hit the ball in the right direction – you also have to hit it with the right amount of power<sup id="fnref:wiigolf"><a href="#fndef:wiigolf" class="fnref">[5]</a></sup>.</p>
<blockquote>
<p>📝 The intuition for this next step in extending the model is much clearer if we plot the new data on top of the fit to the old data, as is done in the Stan documentation. I think this helps to illustrate that although posterior predictive checking can be very powerful, it shouldn&#39;t be the only thing you rely on.</p>
</blockquote>
<h3 id="an_extended_model_that_accounts_for_how_hard_the_ball_is_hit"><a href="#an_extended_model_that_accounts_for_how_hard_the_ball_is_hit" class="header-anchor">An extended model that accounts for how hard the ball is hit</a></h3>
<p>So we have an extra parameter now, <code>sigma_distance</code>, and additionally we make the assumption that the golfer aims to hit the ball 1 foot past the hole, with a distance tolerance of 3 feet &#40;seriously if you&#39;re lost, read the Stan docs&#41;.</p>
<pre><code class="julia hljs"><span class="hljs-meta">@model</span> <span class="hljs-keyword">function</span> golf_angle_distance_2(x,y,n,r,R,overshot,distance_tolerance)
    sigma_angle ~ truncated(Normal(),<span class="hljs-number">0</span>,<span class="hljs-literal">Inf</span>)
    sigma_distance ~ truncated(Normal(),<span class="hljs-number">0</span>,<span class="hljs-literal">Inf</span>)

    threshold_angle = asin.((R-r) ./ x)

    p_angle = <span class="hljs-number">2</span> .* Phi.(threshold_angle ./ sigma_angle) .- <span class="hljs-number">1</span>
    p_distance = Phi.((distance_tolerance - overshot) ./ ((x .+ overshot) .* sigma_distance)) -
        Phi.(-overshot ./ ((x .+ overshot) .* sigma_distance))
    p = p_angle .* p_distance

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:length(n)
        y[i] ~ Binomial(n[i],p[i])
    <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">return</span> sigma_angle*<span class="hljs-number">180</span>/<span class="hljs-literal">π</span>
<span class="hljs-keyword">end</span>

overshot = <span class="hljs-number">1</span>
distance_tolerance = <span class="hljs-number">3</span>

angle_distance_2_model = golf_angle_distance_2(x_new, y_new, n_new, r, R, overshot, distance_tolerance)
fit_angle_distance_2 = sample(angle_distance_2_model,NUTS(),MCMCThreads(),<span class="hljs-number">2000</span>,<span class="hljs-number">4</span>)</code></pre>
<pre><code class="plaintext hljs">Chains MCMC chain (2000×14×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 25.08 seconds
Compute duration  = 25.04 seconds
parameters        = sigma_angle, sigma_distance
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
      parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   ess_per_sec
          Symbol   Float64   Float64   Float64    Float64    Float64   Float64       Float64

     sigma_angle    0.0138    0.0016    0.0004    37.4918    21.0001    1.3165        1.4972
  sigma_distance    0.1327    0.0127    0.0031    38.0229    28.9432    1.3176        1.5184

Quantiles
      parameters      2.5%     25.0%     50.0%     75.0%     97.5%
          Symbol   Float64   Float64   Float64   Float64   Float64

     sigma_angle    0.0132    0.0133    0.0133    0.0134    0.0196
  sigma_distance    0.0942    0.1366    0.1371    0.1375    0.1383
</code></pre>
<h4 id="a_bad_fit"><a href="#a_bad_fit" class="header-anchor">A bad fit</a></h4>
<p>Now before we go jumping into our posterior predictive checks to try and confirm that our idea was as brilliant as we thought it was, let&#39;s take a quick look at the diagnostics provided by Turing. The ESS is very low and the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span> values for both parameters are much larger than 1, indicated poor mixing. We can plot the chains to see if we can see anything obvious.</p>
<pre><code class="julia hljs">plot(fit_angle_distance_2)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/output/chain_plot.svg" alt="">
<p>The chains are definitely not mixing, and it looks like they are getting stuck in some really tight local minima. The issue is with the Binomial error model, it tries too hard to fit the first few points &#40;due to the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">n_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> being large&#41; and this causes the rest of the fit to be off.</p>
<blockquote>
<p>📝 Again, this illustrates that we don&#39;t always need posterior predictive checks to tell us if something is wrong. In fact, if the problem is obvious, as is the case here, we don&#39;t really need to go through all the effort of setting up the checks.</p>
</blockquote>
<h4 id="a_good_fit"><a href="#a_good_fit" class="header-anchor">A good fit</a></h4>
<p>The solution given is to approximate the Binomial data distribution with a Normal and then add independent variance. This forces &#40;helps?&#41; the model to not fit <em>so</em> well to certain data points to the detriment of others, improving the <em>overall</em> fit.</p>
<pre><code class="julia hljs"><span class="hljs-meta">@model</span> <span class="hljs-keyword">function</span> golf_angle_distance_3(x,y,n,r,R,overshot,distance_tolerance)
    sigma_angle ~ truncated(Normal(),<span class="hljs-number">0</span>,<span class="hljs-literal">Inf</span>)
    sigma_distance ~ truncated(Normal(),<span class="hljs-number">0</span>,<span class="hljs-literal">Inf</span>)    
    sigma_y ~ truncated(Normal(),<span class="hljs-number">0</span>,<span class="hljs-literal">Inf</span>)

    threshold_angle = asin.((R-r) ./ x)
    p_angle = <span class="hljs-number">2</span> .* Phi.(threshold_angle ./ sigma_angle) .- <span class="hljs-number">1</span>
    p_distance = Phi.((distance_tolerance - overshot) ./ ((x .+ overshot) .* sigma_distance)) -
        Phi.(-overshot ./ ((x .+ overshot) .* sigma_distance))
    p = p_angle .* p_distance

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:length(n)
        y[i] ~ Normal(p[i], sqrt(p[i] * (<span class="hljs-number">1</span> - p[i]) / n[i] + sigma_y^<span class="hljs-number">2</span>));
    <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">return</span> sigma_angle*<span class="hljs-number">180</span>/<span class="hljs-literal">π</span>
<span class="hljs-keyword">end</span>

angle_distance_3_model = golf_angle_distance_3(x_new,y,n_new,r,R,overshot,distance_tolerance)
fit_angle_distance_3 = sample(angle_distance_3_model,NUTS(),MCMCThreads(),<span class="hljs-number">2000</span>,<span class="hljs-number">4</span>)</code></pre>
<pre><code class="plaintext hljs">Chains MCMC chain (2000×15×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 1.82 seconds
Compute duration  = 1.78 seconds
parameters        = sigma_angle, sigma_distance, sigma_y
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
      parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ess_per_sec
          Symbol   Float64   Float64   Float64     Float64     Float64   Float64       Float64

     sigma_angle    0.0178    0.0001    0.0000   3587.8373   3538.0850    1.0013     2016.7719
  sigma_distance    0.0801    0.0013    0.0000   3520.0269   3590.7595    1.0016     1978.6548
         sigma_y    0.0030    0.0006    0.0000   3940.3981   3913.3049    1.0007     2214.9512

Quantiles
      parameters      2.5%     25.0%     50.0%     75.0%     97.5%
          Symbol   Float64   Float64   Float64   Float64   Float64

     sigma_angle    0.0176    0.0177    0.0178    0.0179    0.0180
  sigma_distance    0.0775    0.0792    0.0801    0.0810    0.0827
         sigma_y    0.0020    0.0026    0.0030    0.0034    0.0044
</code></pre>
<pre><code class="julia hljs"><span class="hljs-comment"># get posterior median</span>
med_sigma_angle, med_sigma_distance, _ = [quantile(<span class="hljs-built_in">Array</span>(fit_angle_distance_3)[:,i],<span class="hljs-number">0.5</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">3</span>]

threshold_angle = asin.((R-r) ./ x_new)

p_angle = <span class="hljs-number">2</span> .* Phi.(threshold_angle ./ med_sigma_angle) .- <span class="hljs-number">1</span>
p_distance = Phi.((distance_tolerance - overshot) ./ ((x_new .+ overshot) .* med_sigma_distance)) -
    Phi.(-overshot ./ ((x_new .+ overshot) .* med_sigma_distance))
post_line = p_angle .* p_distance

plot(x_new,post_line,color=:blue,legend=:<span class="hljs-literal">false</span>)

scatter!(x_new,y, yerror = yerror_new,
        ylabel = <span class="hljs-string">&quot;Probability of success&quot;</span>,
        xlabel = <span class="hljs-string">&quot;Distance from hole (feet)&quot;</span>,
        title = <span class="hljs-string">&quot;Angle and distance model (good fit)&quot;</span>,
        legend=:<span class="hljs-literal">false</span>,
        color=:blue)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/output/good_fit.svg" alt="">
<p>So far so good. The moment of truth:</p>
<pre><code class="julia hljs"><span class="hljs-comment"># Instantiate the predictive model</span>
angle_distance_3_predict = golf_angle_distance_3(x_new,similar(y, <span class="hljs-built_in">Missing</span>),n_new,r,R,overshot,distance_tolerance)
posterior_predictive = predict(angle_distance_3_predict, fit_angle_distance_3)

<span class="hljs-comment"># Ensure the ordering of the loglikelihoods matches the ordering of `posterior_predictive`</span>
loglikelihoods = pointwise_loglikelihoods(angle_distance_3_model, fit_angle_distance_3)
names = string.(keys(posterior_predictive))
loglikelihoods_vals = getindex.(<span class="hljs-built_in">Ref</span>(loglikelihoods), names)
<span class="hljs-comment"># Reshape into `(nchains, nsamples, size(y)...)`</span>
loglikelihoods_arr = permutedims(cat(loglikelihoods_vals...; dims=<span class="hljs-number">3</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>));

idata_distance = from_mcmcchains(
    fit_angle_distance_3;
    posterior_predictive=posterior_predictive,
    log_likelihood=<span class="hljs-built_in">Dict</span>(<span class="hljs-string">&quot;ll&quot;</span> =&gt; loglikelihoods_arr),
    library=<span class="hljs-string">&quot;Turing&quot;</span>,
    observed_data=(; y)
);
distance_loo = loo(idata_distance,pointwise=<span class="hljs-literal">true</span>);
println(<span class="hljs-string">&quot;LOO is &quot;</span>,round(distance_loo.elpd_loo[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>), <span class="hljs-string">&quot; with an SE of &quot;</span>,round(distance_loo.se[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>),
        <span class="hljs-string">&quot; and an estimated number of parameters of &quot;</span>,round(distance_loo.p_loo[<span class="hljs-number">1</span>],digits=<span class="hljs-number">2</span>))</code></pre>
<pre><code class="plaintext hljs">LOO is 127.41 with an SE of 4.97 and an estimated number of parameters of 4.99
</code></pre>
<pre><code class="julia hljs">plot_loo_pit(idata_distance; y=<span class="hljs-string">&quot;y&quot;</span>)
plot_loo_pit(idata_distance; y=<span class="hljs-string">&quot;y&quot;</span>,ecdf=<span class="hljs-literal">true</span>)
plot_khat(distance_loo)</code></pre>
<img src="/assets/blog/golf-putting-in-turing/code/../images/distance_loo_pit_new.png" alt="">
<img src="/assets/blog/golf-putting-in-turing/code/../images/distance_loo_pit_ecdf_new.png" alt="">
<img src="/assets/blog/golf-putting-in-turing/code/../images/distance_khat_new.png" alt="">
<h2 id="model_selection"><a href="#model_selection" class="header-anchor">Model selection</a></h2>
<p>One final thing we can do, although it&#39;s more or less redundant in this example, is to compare our models based on PSIS-LOO. In order to do this we can use the <code>compare</code> function. It takes a <code>Dict</code> of <code>InferenceData</code> objects as input, and returns a <code>DataFrame</code> ordered from best to worst model.</p>
<blockquote>
<p>📝 Some of our models were fit to different datasets – to do model comparison that actually makes sense, the different models have to be fit to the same data. I won&#39;t show the code for this but the models have been re-fit to the new data.</p>
</blockquote>

<pre><code class="julia hljs">comparison_dict = <span class="hljs-built_in">Dict</span>(<span class="hljs-string">&quot;logistic&quot;</span> =&gt; idata_logistic,
                       <span class="hljs-string">&quot;angle&quot;</span> =&gt; idata_angle,
                       <span class="hljs-string">&quot;angle_distance_3&quot;</span> =&gt; idata_distance)
compare(comparison_dict)</code></pre>
<pre><code class="plaintext hljs">3×10 DataFrame
 Row │ name              rank   elpd_loo        p_loo       elpd_diff  weight       se           dse      warning  scale
     │ String            Int64  Float64         Float64     Float64    Float64      Float64      Float64  Bool     String
─────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ angle_distance_3      0     127.407         4.98757        0.0  1.0              4.96743      0.0     true  log
   2 │ logistic              1  -72595.0        3074.42       72722.4  0.0          15933.6      15934.5     true  log
   3 │ angle                 2      -1.90037e5     0.16127   190164.0  2.12192e-10  67980.4      67978.4     true  log</code></pre>
<h2 id="conclusion"><a href="#conclusion" class="header-anchor">Conclusion</a></h2>
<p>If you&#39;ve been wanting to do posterior predictive checks with your models in Turing and didn&#39;t know where to start, hopefully you&#39;ve learnt something from this post and will be able to integrate these tools into your workflow.</p>
<p>Thank you to all the hard work of the Turing and ArviZ developers, as well as everyone I&#39;ve referenced here, and thanks to <a href="https://jokroese.com/">Jo Kroese</a> for helping me with the CSS, so now the images are actually big enough to see.</p>
<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<p><table class="fndef" id="fndef:jduncan">
    <tr>
        <td class="fndef-backref"><a href="#fnref:jduncan">[1]</a></td>
        <td class="fndef-content"><a href="https://jduncstats.com/posts/2019-11-02-golf-turing/">https://jduncstats.com/posts/2019-11-02-golf-turing/</a></td>
    </tr>
</table>
<table class="fndef" id="fndef:elpd">
    <tr>
        <td class="fndef-backref"><a href="#fnref:elpd">[2]</a></td>
        <td class="fndef-content">Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. <a href="https://arxiv.org/abs/1507.04544v5">https://arxiv.org/abs/1507.04544v5</a></td>
    </tr>
</table>
 <table class="fndef" id="fndef:loopit">
    <tr>
        <td class="fndef-backref"><a href="#fnref:loopit">[3]</a></td>
        <td class="fndef-content"><a href="https://oriolabrilpla.cat/python/arviz/pymc3/2019/07/31/loo-pit-tutorial.html">https://oriolabrilpla.cat/python/arviz/pymc3/2019/07/31/loo-pit-tutorial.html</a></td>
    </tr>
</table>
 <table class="fndef" id="fndef:psisdiag">
    <tr>
        <td class="fndef-backref"><a href="#fnref:psisdiag">[4]</a></td>
        <td class="fndef-content">Using the loo package &#40;PSIS diagnostic plots&#41;. <a href="https://mc-stan.org/loo/articles/loo2-example.html">https://mc-stan.org/loo/articles/loo2-example.html</a></td>
    </tr>
</table>
 <table class="fndef" id="fndef:wiigolf">
    <tr>
        <td class="fndef-backref"><a href="#fnref:wiigolf">[5]</a></td>
        <td class="fndef-content">Anyone who&#39;s played Wii Sports Resort golf knows this <a href="https://www.youtube.com/shorts/gaIIr_mPnj0">all too well</a>.</td>
    </tr>
</table>
       <script src="https://utteranc.es/client.js"
              repo="burtonjosh/burtonjosh.github.io"
              issue-term="url"
              label="comments"
              theme="github-light"
              crossorigin="anonymous"
              async>
      </script>
</p>
<!-- <div class="page-foot">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> {{ fill author }}. {{isnotpage /tag/*}}Last modified: {{ fill fd_mtime }}.{{end}}
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div> -->
</div><!-- CONTENT ENDS HERE -->

      </div> <!-- closure of main -->
    </div>   <!-- closure of class initial--content -->

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
        <!-- end custom footer snippets -->
        <div class="page__footer-follow">
          <ul class="social-icons">
            <li><strong>Follow:</strong></li>
            <li><a href="https://twitter.com/josh_burt0n" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
            <li><a href="https://github.com/burtonjosh" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          </ul>
        </div>
        <div class="page__footer-copyright">&copy; Joshua Burton. Powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>,  <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>
          and the <a href="https://julialang.org">Julia programming language</a>.</div>
      </footer>
    </div>

    <script src="/libs/minimal-mistakes/main.min.js"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>

    
        



    
    
        


    
  </body>
</html>
